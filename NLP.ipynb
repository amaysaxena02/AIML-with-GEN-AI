{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amaysaxena02/AIML-with-GEN-AI/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP -\n",
        "1. Tokenization\n",
        "2. Embedding (Word2Vec, Glove)\n",
        "3. Bag-of-Word and TF-IDP"
      ],
      "metadata": {
        "id": "kbwUQikWBGiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP - Natural Language Processing is a branch of Artificial Intelligence that helps computer to understand, interprete, generate human languaga.\n",
        "\n",
        "NLP Applications - Search Engine, Chatbots, Email Filtering, Speech to text, Grammer correction, Sentiment Analysis.\n",
        "\n",
        "NLP Steps -\n",
        "\n",
        "---\n",
        "Tokenization -\n",
        "Remove Stopwords\n",
        "Stemming\n",
        "Vectorization\n",
        "Model Prediction\n",
        "\n",
        "Basic Input -\n",
        "\"NLP is the best branch of AI\"\n",
        "\"NLP\"\"is\"\"the\"\"best\"\"brance\"\"of\"\"AI\""
      ],
      "metadata": {
        "id": "z6OdDWldB3bt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVe94HnB_uqU",
        "outputId": "d27e1dde-087e-4dc8-c864-9f1ec6cb575f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "’\n",
            "m\n",
            "thrilled\n",
            "to\n",
            "share\n",
            "that\n",
            "I\n",
            "’\n",
            "ve\n",
            "earned\n",
            "the\n",
            "50\n",
            "Days\n",
            "Badge\n",
            "2025\n",
            "on\n",
            "LeetCode\n",
            "!\n",
            "This\n",
            "represents\n",
            "50+\n",
            "days\n",
            "of\n",
            "consistent\n",
            "problem-solving\n",
            ",\n",
            "learning\n",
            ",\n",
            "and\n",
            "growing\n",
            "in\n",
            "the\n",
            "world\n",
            "of\n",
            "data\n",
            "structures\n",
            "and\n",
            "algorithms\n",
            ".\n",
            "40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk #nltk is the python librabry that is used for working in human language data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "from nltk.tokenize import word_tokenize\n",
        "data = \"I’m thrilled to share that I’ve earned the 50 Days Badge 2025 on LeetCode! This represents 50+ days of consistent problem-solving, learning, and growing in the world of data structures and algorithms.\"\n",
        "Processed_Data = word_tokenize(data)\n",
        "for i in Processed_Data:\n",
        "  print(i)\n",
        "print(len(Processed_Data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "Data1 = set(stopwords.words('english'))\n",
        "data2 = []\n",
        "for i in Processed_Data:\n",
        "  if i.lower() not in Data1:\n",
        "    data2.append(Processed_Data)\n",
        "print(len(data2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxJUq2JzIhCd",
        "outputId": "01d9a38f-a0b2-421e-a4b4-f02528fdbc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming -\n",
        "Reduce word to there base root or form"
      ],
      "metadata": {
        "id": "xlf1euuyPeXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "Stemmer = PorterStemmer()\n",
        "data3 = []\n",
        "for i in Processed_Data:\n",
        "  data3.append(Stemmer.stem(i))\n",
        "print(data3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-7GutzgNQzw",
        "outputId": "e2581977-7f55-4f19-f574-59cc9ed4b928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', '’', 'm', 'thrill', 'to', 'share', 'that', 'i', '’', 've', 'earn', 'the', '50', 'day', 'badg', '2025', 'on', 'leetcod', '!', 'thi', 'repres', '50+', 'day', 'of', 'consist', 'problem-solv', ',', 'learn', ',', 'and', 'grow', 'in', 'the', 'world', 'of', 'data', 'structur', 'and', 'algorithm', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorization -\n",
        "It is a techniques that convert text into a numeric vector\n",
        "#Type of Vectorization -\n",
        "1. Bag of Words (BoW) - Bag of words count a word how many times appears in a whole documents\n",
        "2. TF-IDF\n",
        "3. Embedding"
      ],
      "metadata": {
        "id": "69BTk15XU0ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that enables computers to understand, interpret, manipulate, and generate human language.\", \"It's an interdisciplinary field that combines computer science, AI, and linguistics to process large amounts of text and speech data.\"]\n",
        "\n",
        "Vector = CountVectorizer()\n",
        "New_text = Vector.fit_transform(text)\n",
        "print(Vector.get_feature_names_out())\n",
        "print(New_text.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MHhmDmCRQTi",
        "outputId": "50e4fad1-b1d4-4cb6-8fa9-0b56555b81d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'amounts' 'an' 'and' 'artificial' 'branch' 'combines' 'computer'\n",
            " 'computers' 'data' 'enables' 'field' 'generate' 'human' 'intelligence'\n",
            " 'interdisciplinary' 'interpret' 'is' 'it' 'language' 'large'\n",
            " 'linguistics' 'manipulate' 'natural' 'nlp' 'of' 'process' 'processing'\n",
            " 'science' 'speech' 'text' 'that' 'to' 'understand']\n",
            "[[1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 2 0 0 1 1 1 1 0 1 0 0 0 1 1 1]\n",
            " [1 1 1 2 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IF - TDF\n",
        "\n",
        "IF - How frequently they appear in the whole documnet\n",
        "\n",
        "IDF - how rear they are in the whole document"
      ],
      "metadata": {
        "id": "UeMV_Zg8aXXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "vectors = tfidf.fit_transform(text)\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(vectors.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ZtYNu_XFed",
        "outputId": "f2b87a86-1fe1-4f2b-fa02-7191a22b031d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'amounts' 'an' 'and' 'artificial' 'branch' 'combines' 'computer'\n",
            " 'computers' 'data' 'enables' 'field' 'generate' 'human' 'intelligence'\n",
            " 'interdisciplinary' 'interpret' 'is' 'it' 'language' 'large'\n",
            " 'linguistics' 'manipulate' 'natural' 'nlp' 'of' 'process' 'processing'\n",
            " 'science' 'speech' 'text' 'that' 'to' 'understand']\n",
            "[[0.15702636 0.         0.         0.15702636 0.22069507 0.22069507\n",
            "  0.         0.         0.22069507 0.         0.22069507 0.\n",
            "  0.22069507 0.22069507 0.22069507 0.         0.22069507 0.22069507\n",
            "  0.         0.44139013 0.         0.         0.22069507 0.22069507\n",
            "  0.22069507 0.15702636 0.         0.22069507 0.         0.\n",
            "  0.         0.15702636 0.15702636 0.22069507]\n",
            " [0.16747189 0.23537589 0.23537589 0.33494377 0.         0.\n",
            "  0.23537589 0.23537589 0.         0.23537589 0.         0.23537589\n",
            "  0.         0.         0.         0.23537589 0.         0.\n",
            "  0.23537589 0.         0.23537589 0.23537589 0.         0.\n",
            "  0.         0.16747189 0.23537589 0.         0.23537589 0.23537589\n",
            "  0.23537589 0.16747189 0.16747189 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding -\n",
        "Word Embedding is mapped each word to a vector of real number\n",
        "* Types of Embedding -\n",
        "1. Word2Vec\n",
        "2. Glove"
      ],
      "metadata": {
        "id": "N2QqCokJeEv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "#model1 = api.load(\"word2vec-google-news-300\") (Very Big Data)\n",
        "model1 = api.load(\"glove-wiki-gigaword-50\")\n",
        "print(model1)"
      ],
      "metadata": {
        "id": "1gDqyrM3f16C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "5481d134-c56d-4380-be40-6678f36960ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-68-2510266605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model1 = api.load(\"word2vec-google-news-300\") (Very Big Data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-wiki-gigaword-50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model1['computer']\n",
        "print(vector[:10])"
      ],
      "metadata": {
        "id": "2RBW2xrtf_QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simi = model1.most_similar(\"computer\", topn = 5)\n",
        "for i in simi:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rR4uV4MBh_GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "! pip install torch # installing pytorch"
      ],
      "metadata": {
        "id": "6QxoOFh4jEZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989bb7b6-5511-4466-a469-cdb5cc6ce6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as fn"
      ],
      "metadata": {
        "id": "HNFDjs0-X12w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['The','Man','Work']\n",
        "print(text)\n",
        "\n",
        "e_d = 8\n",
        "\n",
        "w_idx = {i: index for index, i in enumerate(text)}\n",
        "print(w_idx)\n",
        "\n",
        "idx_w = torch.tensor([w_idx[i] for i in text])\n",
        "print(idx_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIm8y0mPZ4I1",
        "outputId": "fa68263a-3c0d-4ec3-ef84-b3e169df3151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Man', 'Work']\n",
            "{'The': 0, 'Man': 1, 'Work': 2}\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = nn.Embedding(num_embeddings=len(w_idx), embedding_dim=e_d)"
      ],
      "metadata": {
        "id": "y7Hudkglazcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeded = embed(idx_w)\n",
        "print(embeded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdJuCr_-bXnQ",
        "outputId": "9619c7cd-fbd2-4a47-97de-e2b5c66fe1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3772,  1.3685,  0.2580, -0.8880,  0.6046,  0.5787,  0.0463, -0.5394],\n",
            "        [-1.5854,  0.7856,  0.3260, -0.1192, -1.6070,  0.7428, -0.1742, -0.0860],\n",
            "        [ 0.6454, -0.9384, -0.2238,  1.5959,  0.6482, -0.3121,  0.4791,  1.1466]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeded = embeded.unsqueeze(0)\n",
        "embeded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLoDvhsQcBbR",
        "outputId": "74e36b78-f725-4a50-e2ae-27988911c7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3772,  1.3685,  0.2580, -0.8880,  0.6046,  0.5787,  0.0463,\n",
              "          -0.5394],\n",
              "         [-1.5854,  0.7856,  0.3260, -0.1192, -1.6070,  0.7428, -0.1742,\n",
              "          -0.0860],\n",
              "         [ 0.6454, -0.9384, -0.2238,  1.5959,  0.6482, -0.3121,  0.4791,\n",
              "           1.1466]]], grad_fn=<UnsqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_head = 2\n",
        "multi_head = nn.MultiheadAttention(embed_dim=e_d, num_heads=num_head, batch_first=True)"
      ],
      "metadata": {
        "id": "cub6ECc5ci7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkPXAZx8dbsc",
        "outputId": "c31f1f17-35cd-4ea0-8281-da762903da42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiheadAttention(\n",
              "  (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_output, a_weights = multi_head(embeded, embeded, embeded)"
      ],
      "metadata": {
        "id": "kRKoFKMWdiHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUrmMrk2eFUb",
        "outputId": "56c58992-058a-4e47-a476-52ebea9e96b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0312, -0.0087,  0.0975, -0.0490, -0.1841,  0.0919,  0.0093,\n",
              "          -0.0648],\n",
              "         [-0.0158,  0.1058, -0.0168, -0.0523,  0.0583,  0.0531,  0.0718,\n",
              "          -0.0415],\n",
              "         [-0.0456,  0.0385,  0.0034, -0.0162,  0.0314, -0.0148,  0.0296,\n",
              "          -0.0790]]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Aof2SDbejCw",
        "outputId": "dfad1d35-bfba-41ba-c393-a557d722817e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3218, 0.3578, 0.3203],\n",
              "         [0.2245, 0.3819, 0.3936],\n",
              "         [0.4194, 0.2872, 0.2934]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Attention_weights = a_weights.squeeze(0).detach().numpy()\n",
        "print(Attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud_dYYLkeke5",
        "outputId": "6871f61d-b529-47ce-d47f-0ee5d7ac2d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.32182395 0.3578454  0.32033065]\n",
            " [0.2245382  0.3818835  0.39357835]\n",
            " [0.4194068  0.2871827  0.2934105 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_output.squeeze(0).detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izqpV-WofrWT",
        "outputId": "5bb0a1ea-9cde-40e3-d788-10e066ee3c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.03120146, -0.0087449 ,  0.0975014 , -0.04899884, -0.18407789,\n",
              "         0.09187293,  0.00926037, -0.06475873],\n",
              "       [-0.01582786,  0.1057599 , -0.01679613, -0.05232883,  0.05833446,\n",
              "         0.05314153,  0.07177547, -0.04154338],\n",
              "       [-0.04555234,  0.03845286,  0.0033769 , -0.01624708,  0.03137161,\n",
              "        -0.01481414,  0.02961339, -0.07897667]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "model = 'Helsinki-NLP/opus-mt-en-hi'\n",
        "tok = MarianTokenizer.from_pretrained(model)\n",
        "models = MarianMTModel.from_pretrained(model)"
      ],
      "metadata": {
        "id": "SQVpn718m1EI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eee5a61-6274-4020-8c67-2fdec3926b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = ['dog and cats are pets']\n",
        "\n",
        "tok_input = tok(input_text,return_tensors='pt', padding = True, truncation= True)\n",
        "tok_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA8sSMhKsN-B",
        "outputId": "493951a6-f92d-4d0b-c51e-9fcce0f36046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 8195,    10, 33636,    54, 21051,    16,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate = models.generate(**tok_input)\n",
        "translate\n",
        "output = tok.batch_decode(translate, skip_skip_special_tokens=True)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hO4RBoqqzAe",
        "outputId": "49cf332c-0cf4-4169-a2a9-e83fc422da7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad> कुत्ता और बैल पालतू हैं</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numpy, pandas, matplotlib, seaborn, scikitlearn\n",
        "\n",
        "\n",
        "pytorch, tensoreflow, tensoreflow.keras, nltk, transformer (huggingface library part), Gensim, langdetect, opencv, PIL (pillow), torchvision  \n",
        "\n",
        "\n",
        "read about fast ai which is used for high level deep learning and pytorch"
      ],
      "metadata": {
        "id": "iyS2iA2hsnVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task: Face mask detection using cnn + opencv"
      ],
      "metadata": {
        "id": "PsVYs8_4t_Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-sU-tYW5sazC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}